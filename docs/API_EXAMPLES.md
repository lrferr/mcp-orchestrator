# MCP Orchestrator API - Ollama Integration Endpoints

## Ollama Integration Module

The project includes specific endpoints for checking and listing available Ollama models.

> **üìö Complete Documentation:** See [API_ENDPOINTS.md](API_ENDPOINTS.md) for full API documentation.

### Endpoints Adicionados:

#### 1. **Listar Modelos do Ollama**
```
GET /api/mcp/ollama/models
```

**Exemplo de resposta:**
```json
{
  "total": 3,
  "models": [
    {
      "name": "llama3:latest",
      "size": 4808364084,
      "modifiedAt": "2024-01-15T10:30:00.000Z",
      "format": "gguf",
      "family": "llama3",
      "parameterSize": "8B",
      "quantizationLevel": "Q4_0"
    },
    {
      "name": "mistral:7b",
      "size": 4214570084,
      "modifiedAt": "2024-01-10T08:45:00.000Z",
      "format": "gguf",
      "family": "mistral",
      "parameterSize": "7B",
      "quantizationLevel": "Q4_0"
    }
  ]
}
```

#### 2. **Obter Informa√ß√µes Detalhadas sobre um Modelo Espec√≠fico**
```
GET /api/mcp/ollama/models/{modelName}
```

**Exemplo:**
```
GET /api/mcp/ollama/models/llama3:latest
```

**Exemplo de resposta:**
```json
{
  "model": {
    "name": "llama3:latest",
    "size": 4808364084,
    "modifiedAt": "2024-01-15T10:30:00.000Z",
    "format": "gguf",
    "family": "llama3",
    "parameterSize": "8B",
    "quantizationLevel": "Q4_0",
    "template": "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n{{ .System }}<|eot_id|>",
    "parameters": "num_predict 50 temperature 0.4 top_k 40 top_p 0.9"
  }
}
```

#### 3. **Testar Conex√£o com Ollama**
```
GET /api/mcp/ollama/test-connection
```

**Exemplo de resposta (sucesso):**
```json
{
  "status": "SUCCESS",
  "baseUrl": "http://localhost:11434",
  "message": "Connection to Ollama successful"
}
```

**Exemplo de resposta (erro):**
```json
{
  "status": "ERROR",
  "baseUrl": "http://localhost:11434",
  "message": "Failed to connect to Ollama: Connection refused"
}
```

## Como Usar no Swagger UI

1. **Acesse o Swagger UI**: `http://localhost:8080/swagger-ui.html`
2. **Navegue at√© a se√ß√£o "McpApi"**
3. **Encontre os endpoints "ollama" para intera√ß√£o com modelos**
4. **Use o bot√£o "Try it out" para testar os endpoints**

## Configura√ß√£o

O Ollama application properties j√° est√° configurado no `application.properties`:

```properties
# Configura√ß√£o do Ollama
spring.ai.ollama.base-url=http://localhost:11434/
spring.ai.ollama.chat.model=llama3
```

## Arquitetura

O novo m√≥dulo inclui:

1. **`OllamaService.java`** - Servi√ßo para intera√ß√£o com API do Ollama
2. **Endpoints atualizados no `McpServerController.java`** para integra√ß√£o com Ollama
3. **Tratamento de exce√ß√µes completo** para cen√°rios offline/disconectado do Ollama
4. **Mapeamento detalhado** de informa√ß√µes dos modelos (tamanho, formato, fam√≠lia, etc.)

## Benef√≠cios

- ‚úÖ **Verifica√ß√£o r√°pida** dos modelos dispon√≠veis via API
- ‚úÖ **Informa√ß√µes detalhadas** sobre cada modelo
- ‚úÖ **Monitoramento da conex√£o** com o servidor Ollama
- ‚úÖ **Integra√ß√£o direta** com a API REST existente no Swagger UI
- ‚úÖ **Tratamento de erros robusto** quando Ollama n√£o est√° dispon√≠vel
